{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable Artificial Intelligence Sample Project\n",
    "####  8th February 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explainable Artificial Intelligence (XAI) or Interpretable AI refers to methods that aim at making Artificial Intelligence (AI) systems more transparent, such that their decisions can be understood by humans. It is the opposite of the concept of \"black box\", where even the AI developers cannot explain why the AI system arrived at a specific decision. We can think of XAI as in implementation of the social right to explanation. \n",
    "\n",
    "Here, we are going to use deep learning to classify a chosen text as positive, negative or neutral, on the basis of its content. After the classification, a method called LRP (Layer-wise Relevance Propagation) will be used to explain why the deep learning system came to its decision, making it an Explainable Artificial Intelligence system (XAI) instead of a black-box system.\n",
    "\n",
    "The LRP implementation is based on the following papers:\n",
    "- [https://doi.org/10.1371/journal.pone.0130140](https://doi.org/10.1371/journal.pone.0130140)\n",
    "- [https://doi.org/10.18653/v1/W17-5221](https://doi.org/10.18653/v1/W17-5221)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the text \n",
    "First, we will enter the text we want to classify, and make it compatible with the Neural Network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*import libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSTM_bidi import *                      # the LSTM_bidi file contains functions for processing a text, performing \n",
    "                                             # sentiment analysis with deep learning and explaining the result with LRP\n",
    "    \n",
    "from heatmap import html_heatmap             # the heatmap file contains the functions for converting the relevances\n",
    "                                             # obtained by a LRP to readable heatmaps\n",
    "\n",
    "import codecs                                # codecs is a package used to code or decodes text to bytes\n",
    "import numpy as np                           # NumPy is the package for array computing \n",
    "from IPython.display import display, HTML    # IPython.display is a public API for display tools in IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" We will consider both types of predictors in a generic sense, trying to avoid whenever possible a priori restrictions to specific algorithms or mappings. The next Section Pixel-wise Decomposition as a General Concept will explain the basic approaches underlying the pixel-wise decomposition of classifiers. In Section Bag of Words models revisited, we will give a short recapitulation about Bag of Words features and kernel-based classifiers and summarize related work. Overview of the decomposition steps will discuss the decomposition of a kernel-based classifier into sums of scores over small regions of the image, and the projection down to single pixels.  \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'will', 'consider', 'both', 'types', 'of', 'predictors', 'in', 'a', 'generic', 'sense', 'trying', 'to', 'avoid', 'whenever', 'possible', 'a', 'priori', 'restrictions', 'to', 'specific', 'algorithms', 'or', 'mappings', 'the', 'next', 'section', 'pixel-wise', 'decomposition', 'as', 'a', 'general', 'concept', 'will', 'explain', 'the', 'basic', 'approaches', 'underlying', 'the', 'pixel-wise', 'decomposition', 'of', 'classifiers', 'in', 'section', 'bag', 'of', 'words', 'models', 'revisited', 'we', 'will', 'give', 'a', 'short', 'recapitulation', 'about', 'bag', 'of', 'words', 'features', 'and', 'kernel-based', 'classifiers', 'and', 'summarize', 'related', 'work', 'overview', 'of', 'the', 'decomposition', 'steps', 'will', 'discuss', 'the', 'decomposition', 'of', 'a', 'kernel-based', 'classifier', 'into', 'sums', 'of', 'scores', 'over', 'small', 'regions', 'of', 'the', 'image', 'and', 'the', 'projection', 'down', 'to', 'single', 'pixels']\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text = text.replace(\",\", \" \")\n",
    "text = text.replace(\".\", \" \")\n",
    "text = text.split()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'will', 'consider', 'both', 'types', 'of', 'predictors', 'in', 'a', 'generic', 'sense', 'trying', 'to', 'avoid', 'whenever', 'possible', 'a', 'priori', 'restrictions', 'to', 'specific', 'algorithms', 'or', 'mappings', 'the', 'next', 'section', 'pixel-wise', 'decomposition', 'as', 'a', 'general', 'concept', 'will', 'explain', 'the', 'basic', 'approaches', 'underlying', 'the', 'pixel-wise', 'decomposition', 'of', 'classifiers', 'in', 'section', 'bag', 'of', 'words', 'models', 'revisited', 'we', 'will', 'give', 'a', 'short', 'recapitulation', 'about', 'bag', 'of', 'words', 'features', 'and', 'kernel-based', 'classifiers', 'and', 'summarize', 'related', 'work', 'overview', 'of', 'the', 'decomposition', 'steps', 'will', 'discuss', 'the', 'decomposition', 'of', 'a', 'kernel-based', 'classifier', 'into', 'sums', 'of', 'scores', 'over', 'small', 'regions', 'of', 'the', 'image', 'and', 'the', 'projection', 'down', 'to', 'single', 'pixels']\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network is trained on the dataset Stanford Sentiment Treebank (SST), so it only recognizes words that are in the given list. Therefore, our text can not have words that are not in this dataset. A function is defined to remove all the words in the text that are not found in the SST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_words(text):\n",
    "    \"\"\"Removes words from text that are not in the Stanford Sentiment Treebank dataset\"\"\"\n",
    "    net  = LSTM_bidi()         # load in the trained neural network\n",
    "    words = text.copy()        # create a copy of the text\n",
    "    for w in text:             # remove all words that are not in the Standord Sentiment Treebank\n",
    "        if w not in net.voc:\n",
    "            words.remove(w)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = remove_invalid_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'will', 'consider', 'both', 'types', 'of', 'in', 'a', 'generic', 'sense', 'trying', 'to', 'avoid', 'whenever', 'possible', 'a', 'to', 'specific', 'or', 'the', 'next', 'section', 'decomposition', 'as', 'a', 'general', 'concept', 'will', 'explain', 'the', 'basic', 'approaches', 'underlying', 'the', 'decomposition', 'of', 'in', 'section', 'bag', 'of', 'words', 'models', 'revisited', 'we', 'will', 'give', 'a', 'short', 'recapitulation', 'about', 'bag', 'of', 'words', 'features', 'and', 'and', 'related', 'work', 'overview', 'of', 'the', 'decomposition', 'steps', 'will', 'discuss', 'the', 'decomposition', 'of', 'a', 'into', 'sums', 'of', 'scores', 'over', 'small', 'of', 'the', 'image', 'and', 'the', 'projection', 'down', 'to', 'single']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the text classification with a Neural Network\n",
    "Now we are going to classify our text with a Neural Network, more specifically a bidirectional Long Short-Term Memory (LSTM). This is an artificial recurrent neural network (RNN) architecture, which is good for processing not only single data points but also entire sequences of data (such as text or video). It is therefore often used in Natural Language Processing (NLP) projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes in which the text can be classified are defined here. The sentiment classes are encoded the following way: \n",
    "<br> **0 = Very negative, 1 = Negative, 2 = Neutral, 3 = Positive, 4 = Very positive**\n",
    "\n",
    "A list called sentiment_coding is created; it consists of the 5 elements with the text \"Very negative\", \"Negative\" etc. in the order from 0-4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_coding = [\"Very negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very positive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined another function which uses the Neural Network LSTM (LSTM_bidi()). We call the LSTM Neural Network *net*, and the network is already trained on the Stanford Sentiment Treebank (SST) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(words):\n",
    "    \"\"\"Returns the classifier's predicted class\"\"\"\n",
    "    net                 = LSTM_bidi()                                   # load trained LSTM model\n",
    "    w_indices           = [net.voc.index(w) for w in words]             # convert input sentence to word IDs\n",
    "    net.set_input(w_indices)                                            # set LSTM input sequence\n",
    "    scores              = net.forward()                                 # classification prediction scores\n",
    "    return np.argmax(scores)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the sentiment of the text (now called *words*) by calling the function \"predict\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "predicted_class =  predict(words)                                                   # get predicted class\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very negative\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_coding[predicted_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain the text classification by computing LRP relevances\n",
    "Now, we need to find out **why** the neural network came to this decision, and we will use a method called Layer-wise Relevance Propagation (LRP). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning**: We will tune hyperparameters *eps* or *bias_factor*. \n",
    "<br> *eps* is a threshold value for how large the relevance for the words needs to be in order to be shown. If their relevance is below eps, they are set to zero. \n",
    "<br> *bias_factor* is the size of the bias included in the calculation of the relevance. It is recommended to set this as 0, as this leads to that the total amount of relevance is conserved for each layer of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRP hyperparameters:\n",
    "eps                 = 0.001                                                  # small positive number\n",
    "bias_factor         = 0.0                                                    # recommended value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net  = LSTM_bidi()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs the LRP on the classification done by the neural network on the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.76898674  2.33646075  0.36425571 -1.42664486 -3.3979334 ]\n"
     ]
    }
   ],
   "source": [
    "w_indices           = [net.voc.index(w) for w in words]                      # convert input sentence to word IDs\n",
    "Rx, Rx_rev, R_rest  = net.lrp(w_indices, predicted_class, eps, bias_factor)  # perform LRP\n",
    "R_words             = np.sum(Rx + Rx_rev, axis=1)                            # compute word-level LRP relevances\n",
    "scores              = net.s.copy()                                           # classification prediction scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the predicted class of the text (0-4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.76898674  2.33646075  0.36425571 -1.42664486 -3.3979334 ]\n"
     ]
    }
   ],
   "source": [
    "#scores=scores.round()\n",
    "#scores=abs(scores)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prints out the relevance of each word in you text has in the classification. Words marked in red are words that contributed to the predicted class, while words marked in blue are words that had a negative contribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LRP heatmap:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ececff\">we</span> <span style=\"background-color:#ececff\">will</span> <span style=\"background-color:#ffdcdc\">consider</span> <span style=\"background-color:#f2f2ff\">both</span> <span style=\"background-color:#f6f6ff\">types</span> <span style=\"background-color:#fefeff\">of</span> <span style=\"background-color:#fefeff\">in</span> <span style=\"background-color:#fcfcff\">a</span> <span style=\"background-color:#ffacac\">generic</span> <span style=\"background-color:#fefeff\">sense</span> <span style=\"background-color:#ffe4e4\">trying</span> <span style=\"background-color:#fffefe\">to</span> <span style=\"background-color:#fcfcff\">avoid</span> <span style=\"background-color:#ff9393\">whenever</span> <span style=\"background-color:#fafaff\">possible</span> <span style=\"background-color:#f8f8ff\">a</span> <span style=\"background-color:#fffefe\">to</span> <span style=\"background-color:#f0f0ff\">specific</span> <span style=\"background-color:#fff4f4\">or</span> <span style=\"background-color:#fefeff\">the</span> <span style=\"background-color:#fcfcff\">next</span> <span style=\"background-color:#fffefe\">section</span> <span style=\"background-color:#ffacac\">decomposition</span> <span style=\"background-color:#fffefe\">as</span> <span style=\"background-color:#fcfcff\">a</span> <span style=\"background-color:#fafaff\">general</span> <span style=\"background-color:#fefeff\">concept</span> <span style=\"background-color:#fafaff\">will</span> <span style=\"background-color:#fffefe\">explain</span> <span style=\"background-color:#fefeff\">the</span> <span style=\"background-color:#fffafa\">basic</span> <span style=\"background-color:#f6f6ff\">approaches</span> <span style=\"background-color:#fffefe\">underlying</span> <span style=\"background-color:#fefeff\">the</span> <span style=\"background-color:#ffcccc\">decomposition</span> <span style=\"background-color:#fefeff\">of</span> <span style=\"background-color:#fefeff\">in</span> <span style=\"background-color:#fffefe\">section</span> <span style=\"background-color:#fefeff\">bag</span> <span style=\"background-color:#fefeff\">of</span> <span style=\"background-color:#fcfcff\">words</span> <span style=\"background-color:#f8f8ff\">models</span> <span style=\"background-color:#fffcfc\">revisited</span> <span style=\"background-color:#fefeff\">we</span> <span style=\"background-color:#fcfcff\">will</span> <span style=\"background-color:#f2f2ff\">give</span> <span style=\"background-color:#fcfcff\">a</span> <span style=\"background-color:#fff0f0\">short</span> <span style=\"background-color:#e8e8ff\">recapitulation</span> <span style=\"background-color:#fffcfc\">about</span> <span style=\"background-color:#fefeff\">bag</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fefeff\">words</span> <span style=\"background-color:#f3f3ff\">features</span> <span style=\"background-color:#fcfcff\">and</span> <span style=\"background-color:#fcfcff\">and</span> <span style=\"background-color:#fcfcff\">related</span> <span style=\"background-color:#f8f8ff\">work</span> <span style=\"background-color:#eeeeff\">overview</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fffcfc\">the</span> <span style=\"background-color:#ff8888\">decomposition</span> <span style=\"background-color:#f6f6ff\">steps</span> <span style=\"background-color:#f0f0ff\">will</span> <span style=\"background-color:#dcdcff\">discuss</span> <span style=\"background-color:#fffefe\">the</span> <span style=\"background-color:#ff0000\">decomposition</span> <span style=\"background-color:#fafaff\">of</span> <span style=\"background-color:#e8e8ff\">a</span> <span style=\"background-color:#fff8f8\">into</span> <span style=\"background-color:#fefeff\">sums</span> <span style=\"background-color:#fefeff\">of</span> <span style=\"background-color:#b0b0ff\">scores</span> <span style=\"background-color:#fffcfc\">over</span> <span style=\"background-color:#ececff\">small</span> <span style=\"background-color:#fafaff\">of</span> <span style=\"background-color:#f6f6ff\">the</span> <span style=\"background-color:#dadaff\">image</span> <span style=\"background-color:#dedeff\">and</span> <span style=\"background-color:#fafaff\">the</span> <span style=\"background-color:#ffeeee\">projection</span> <span style=\"background-color:#ffc8c8\">down</span> <span style=\"background-color:#e3e3ff\">to</span> <span style=\"background-color:#c3c3ff\">single</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"\\nLRP heatmap:\")    \n",
    "display(HTML(html_heatmap(words, R_words)))                                    # display the heat map of relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LRP heatmap:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#f6f6ff\">we</span> <span style=\"background-color:#d0d0ff\">will</span> <span style=\"background-color:#ffecec\">consider</span> <span style=\"background-color:#dadaff\">both</span> <span style=\"background-color:#fefeff\">types</span> <span style=\"background-color:#f3f3ff\">of</span> <span style=\"background-color:#f6f6ff\">in</span> <span style=\"background-color:#f2f2ff\">a</span> <span style=\"background-color:#ff8a8a\">generic</span> <span style=\"background-color:#f0f0ff\">sense</span> <span style=\"background-color:#ffd6d6\">trying</span> <span style=\"background-color:#fefeff\">to</span> <span style=\"background-color:#f0f0ff\">avoid</span> <span style=\"background-color:#ff5959\">whenever</span> <span style=\"background-color:#f0f0ff\">possible</span> <span style=\"background-color:#f2f2ff\">a</span> <span style=\"background-color:#fffefe\">to</span> <span style=\"background-color:#e3e3ff\">specific</span> <span style=\"background-color:#ffecec\">or</span> <span style=\"background-color:#fcfcff\">the</span> <span style=\"background-color:#fafaff\">next</span> <span style=\"background-color:#fffefe\">section</span> <span style=\"background-color:#ff8a8a\">decomposition</span> <span style=\"background-color:#fffcfc\">as</span> <span style=\"background-color:#fcfcff\">a</span> <span style=\"background-color:#f8f8ff\">general</span> <span style=\"background-color:#fefeff\">concept</span> <span style=\"background-color:#fafaff\">will</span> <span style=\"background-color:#fffcfc\">explain</span> <span style=\"background-color:#fffefe\">the</span> <span style=\"background-color:#fff8f8\">basic</span> <span style=\"background-color:#f3f3ff\">approaches</span> <span style=\"background-color:#fffefe\">underlying</span> <span style=\"background-color:#fffefe\">the</span> <span style=\"background-color:#ffcccc\">decomposition</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fffefe\">in</span> <span style=\"background-color:#fffcfc\">section</span> <span style=\"background-color:#fffefe\">bag</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fefeff\">words</span> <span style=\"background-color:#f8f8ff\">models</span> <span style=\"background-color:#fffefe\">revisited</span> <span style=\"background-color:#fefeff\">we</span> <span style=\"background-color:#fcfcff\">will</span> <span style=\"background-color:#f0f0ff\">give</span> <span style=\"background-color:#fffefe\">a</span> <span style=\"background-color:#fff4f4\">short</span> <span style=\"background-color:#dcdcff\">recapitulation</span> <span style=\"background-color:#fffcfc\">about</span> <span style=\"background-color:#fffcfc\">bag</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fffefe\">words</span> <span style=\"background-color:#f6f6ff\">features</span> <span style=\"background-color:#fefeff\">and</span> <span style=\"background-color:#fffefe\">and</span> <span style=\"background-color:#fcfcff\">related</span> <span style=\"background-color:#fefeff\">work</span> <span style=\"background-color:#f0f0ff\">overview</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fffcfc\">the</span> <span style=\"background-color:#ffacac\">decomposition</span> <span style=\"background-color:#fffefe\">steps</span> <span style=\"background-color:#f8f8ff\">will</span> <span style=\"background-color:#e3e3ff\">discuss</span> <span style=\"background-color:#fffefe\">the</span> <span style=\"background-color:#ff0000\">decomposition</span> <span style=\"background-color:#fefeff\">of</span> <span style=\"background-color:#f3f3ff\">a</span> <span style=\"background-color:#fffafa\">into</span> <span style=\"background-color:#fff8f8\">sums</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#a6a6ff\">scores</span> <span style=\"background-color:#fff2f2\">over</span> <span style=\"background-color:#fefeff\">small</span> <span style=\"background-color:#fcfcff\">of</span> <span style=\"background-color:#fcfcff\">the</span> <span style=\"background-color:#e2e2ff\">image</span> <span style=\"background-color:#e0e0ff\">and</span> <span style=\"background-color:#fcfcff\">the</span> <span style=\"background-color:#ffdcdc\">projection</span> <span style=\"background-color:#ffcece\">down</span> <span style=\"background-color:#f2f2ff\">to</span> <span style=\"background-color:#c6c6ff\">single</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_class = 1\n",
    "\n",
    "w_indices           = [net.voc.index(w) for w in words]                      # convert input sentence to word IDs\n",
    "Rx, Rx_rev, R_rest  = net.lrp(w_indices, target_class, eps, bias_factor)     # perform LRP\n",
    "R_words             = np.sum(Rx + Rx_rev, axis=1)                            # compute word-level LRP relevances\n",
    "\n",
    "scores              = net.s.copy()                                           # classification prediction scores\n",
    "print (\"\\nLRP heatmap:\")    \n",
    "display(HTML(html_heatmap(words, R_words)))                                  # display the heat map of relevances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendations for future work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** LRP can be used on more AI systems than sentiment analysis. I love this demo, where one can use the same implementation of LRP on Handwriting Classification, Image Classification and Text Classification: https://lrpserver.hhi.fraunhofer.de/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Use the LRP to identify *important* words, then play with the LRP by removing those words to see how the prediction changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Try other texts with different sentiment and content. I will try to *fool* the Neural Network with difficult texts, and then use the LRP to see why it made the (wrong) prediction that it did. "
   ]
  }
 ],
 "metadata": {
  "gist_id": "cd29264f4357c587ebca4d2b082df89f",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
