{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable Artificial Intelligence Sample Project\n",
    "####  8th February 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explainable Artificial Intelligence (XAI) refers to methods that aim at making Artificial Intelligence systems (AI) more transparent, such that their decisions can be understood by humans. It it the opposite of the concept of \"black box\", where even the AI developers can't explain why the AI system arrived at a specific decision. We can think if XAI as in implementation of the social right to explanation. \n",
    "\n",
    "In this notebook, we are going to use deep learning to classify a chosen text as positive, negative or neutral, on the basis of it's content. After the classification, we are going to use an method called LRP (Layer-wise Relevance Propagation) to explain why the deep learning system came to it's decision, making it an Explainable Artificial Intelligence system (XAI) instead of a black-box system.\n",
    "\n",
    "The LRP implementation is based on the following papers:\n",
    "- [https://doi.org/10.1371/journal.pone.0130140](https://doi.org/10.1371/journal.pone.0130140)\n",
    "- [https://doi.org/10.18653/v1/W17-5221](https://doi.org/10.18653/v1/W17-5221)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the text \n",
    "First, we are going to enter the text we want to classify, and make it compatible with the Neural Network. We must do a few imports. Make sure the LSTM_bibi.py and heatmap.py files are in the same folder as this file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSTM_bidi import *                      # the LSTM_bidi file containts functions for processing a text, performing \n",
    "                                             # sentiment analysis with deep learning and explaining the result with LRP\n",
    "    \n",
    "from heatmap import html_heatmap             # the heatmap file containt the functions for converting the relevances\n",
    "                                             # obtained by a LRP to readable heatmaps\n",
    "\n",
    "import codecs                                # codecs is a package used to code or decodes text to bytes\n",
    "import numpy as np                           # NumPy is the fundamental package for array computing with Python\n",
    "from IPython.display import display, HTML    # IPython.display is a public API for display tools in IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" We will consider both types of predictors in a generic sense, trying to avoid whenever possible a priori restrictions to specific algorithms or mappings. The next Section Pixel-wise Decomposition as a General Concept will explain the basic approaches underlying the pixel-wise decomposition of classifiers. In Section Bag of Words models revisited, we will give a short recapitulation about Bag of Words features and kernel-based classifiers and summarize related work. Overview of the decomposition steps will discuss the decomposition of a kernel-based classifier into sums of scores over small regions of the image, and the projection down to single pixels.  \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'will', 'consider', 'both', 'types', 'of', 'predictors', 'in', 'a', 'generic', 'sense', 'trying', 'to', 'avoid', 'whenever', 'possible', 'a', 'priori', 'restrictions', 'to', 'specific', 'algorithms', 'or', 'mappings', 'the', 'next', 'section', 'pixel-wise', 'decomposition', 'as', 'a', 'general', 'concept', 'will', 'explain', 'the', 'basic', 'approaches', 'underlying', 'the', 'pixel-wise', 'decomposition', 'of', 'classifiers', 'in', 'section', 'bag', 'of', 'words', 'models', 'revisited', 'we', 'will', 'give', 'a', 'short', 'recapitulation', 'about', 'bag', 'of', 'words', 'features', 'and', 'kernel-based', 'classifiers', 'and', 'summarize', 'related', 'work', 'overview', 'of', 'the', 'decomposition', 'steps', 'will', 'discuss', 'the', 'decomposition', 'of', 'a', 'kernel-based', 'classifier', 'into', 'sums', 'of', 'scores', 'over', 'small', 'regions', 'of', 'the', 'image', 'and', 'the', 'projection', 'down', 'to', 'single', 'pixels']\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text = text.replace(\",\", \" \")\n",
    "text = text.replace(\".\", \" \")\n",
    "text = text.split()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'will', 'consider', 'both', 'types', 'of', 'predictors', 'in', 'a', 'generic', 'sense', 'trying', 'to', 'avoid', 'whenever', 'possible', 'a', 'priori', 'restrictions', 'to', 'specific', 'algorithms', 'or', 'mappings', 'the', 'next', 'section', 'pixel-wise', 'decomposition', 'as', 'a', 'general', 'concept', 'will', 'explain', 'the', 'basic', 'approaches', 'underlying', 'the', 'pixel-wise', 'decomposition', 'of', 'classifiers', 'in', 'section', 'bag', 'of', 'words', 'models', 'revisited', 'we', 'will', 'give', 'a', 'short', 'recapitulation', 'about', 'bag', 'of', 'words', 'features', 'and', 'kernel-based', 'classifiers', 'and', 'summarize', 'related', 'work', 'overview', 'of', 'the', 'decomposition', 'steps', 'will', 'discuss', 'the', 'decomposition', 'of', 'a', 'kernel-based', 'classifier', 'into', 'sums', 'of', 'scores', 'over', 'small', 'regions', 'of', 'the', 'image', 'and', 'the', 'projection', 'down', 'to', 'single', 'pixels']\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Neural Network is trained on the dataset Stanford Sentiment Treebank (SST), it only recognizes words that are in this list. Therefore, our text can not have words that are not in this dataset. We must define a function remove_invalid_words to remove all words in our text that are not found in the Stanford Sentiment Treebank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_words(text):\n",
    "    \"\"\"Removes all words from text that are not in the Stanford Sentiment Treebank dataset\"\"\"\n",
    "    net  = LSTM_bidi()         # load in the trained neural network\n",
    "    words = text.copy()        # create a copy of the text\n",
    "    for w in text:             # remove all words that are not in the Standord Sentiment Treebank\n",
    "        if w not in net.voc:\n",
    "            words.remove(w)\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = remove_invalid_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'will', 'consider', 'both', 'types', 'of', 'in', 'a', 'generic', 'sense', 'trying', 'to', 'avoid', 'whenever', 'possible', 'a', 'to', 'specific', 'or', 'the', 'next', 'section', 'decomposition', 'as', 'a', 'general', 'concept', 'will', 'explain', 'the', 'basic', 'approaches', 'underlying', 'the', 'decomposition', 'of', 'in', 'section', 'bag', 'of', 'words', 'models', 'revisited', 'we', 'will', 'give', 'a', 'short', 'recapitulation', 'about', 'bag', 'of', 'words', 'features', 'and', 'and', 'related', 'work', 'overview', 'of', 'the', 'decomposition', 'steps', 'will', 'discuss', 'the', 'decomposition', 'of', 'a', 'into', 'sums', 'of', 'scores', 'over', 'small', 'of', 'the', 'image', 'and', 'the', 'projection', 'down', 'to', 'single']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the text classification with a Neural Network\n",
    "Now we are going to classify our text with a Neural Network, more specificly a bidirectional Long short-term memory (LSTM). This is an artificial recurrent neural network (RNN) architecture, which are good at processing not only single data points but also entire sequences of data (such as text or video). It is therefore often used for Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to create the classes in which the text can be classied as. The sentiment classes are encoded the following way:  \n",
    "**0 = Very negative, 1 = Negative, 2 = Neutral, 3 = Positive, 4 = Very positive**\n",
    "\n",
    "Create a list called sentiment_coding that consists of the 5 elements with the text \"Very negative\", \"Negative\" etc. in the right order from 0-4. You can read how to create a list consisting of text-elements (called strings) here: https://www.w3schools.com/python/python_lists.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_coding = [\"Very negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very positive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function *predict* which uses the Neural Network LSTM by calling the function LSTM_bidi(), which we imported in the beginning. We call the LSTM Neural Network *net*, and the network is already trained on the Stanford Sentiment Treebank (SST) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(words):\n",
    "    \"\"\"Returns the classifier's predicted class\"\"\"\n",
    "    net                 = LSTM_bidi()                                   # load trained LSTM model\n",
    "    w_indices           = [net.voc.index(w) for w in words]             # convert input sentence to word IDs\n",
    "    net.set_input(w_indices)                                            # set LSTM input sequence\n",
    "    scores              = net.forward()                                 # classification prediction scores\n",
    "    return np.argmax(scores)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the sentiment of your text (now called *words*) by calling the function \"predict\" defined in the previous box, and name the prediction *predicted_class*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "predicted_class =  predict(words)                                                   # get predicted class\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the predicted class of the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very negative\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_coding[predicted_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain the text classification by computing LRP relevances\n",
    "Now we have used a neural network to get the classification/sentiment of our text. Then, we need to find out why the neural network came to this decision, and here we will use a method called Layer-wise Relevance Propagation. We will start by setting it's hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning**: Here we can tune hyperparameters *eps* or *bias_factor*. *eps* is a threshold value for how large the relevance for the words needs to be in order to be shown. If their relevance is below eps, they are set to zero. *bias_factor* is the size of the bias included in the calculation of the relevance. It is recommended to set this as 0, as this leads to that the total amount of relevance is conserved for each layer of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRP hyperparameters:\n",
    "eps                 = 0.001                                                  # small positive number\n",
    "bias_factor         = 0.0                                                    # recommended value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the trained Neural Network from the import LSTM_bidi() and call it *net*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "net  = LSTM_bidi()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs the LRP on the classification done by the neural network on you text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.76898674  2.33646075  0.36425571 -1.42664486 -3.3979334 ]\n"
     ]
    }
   ],
   "source": [
    "w_indices           = [net.voc.index(w) for w in words]                      # convert input sentence to word IDs\n",
    "Rx, Rx_rev, R_rest  = net.lrp(w_indices, predicted_class, eps, bias_factor)  # perform LRP\n",
    "R_words             = np.sum(Rx + Rx_rev, axis=1)                            # compute word-level LRP relevances\n",
    "scores              = net.s.copy()                                           # classification prediction scores\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the predicted class of the text (0-4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.76898674  2.33646075  0.36425571 -1.42664486 -3.3979334 ]\n"
     ]
    }
   ],
   "source": [
    "#scores=scores.round()\n",
    "#scores=abs(scores)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prints out the relevance of each word in you text has in the classification. Words marked in red are words that contributed to the predicted class, while words marked in blue are words that had a negative contribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LRP heatmap:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#f6f6ff\">we</span> <span style=\"background-color:#d0d0ff\">will</span> <span style=\"background-color:#ffecec\">consider</span> <span style=\"background-color:#dadaff\">both</span> <span style=\"background-color:#fefeff\">types</span> <span style=\"background-color:#f3f3ff\">of</span> <span style=\"background-color:#f6f6ff\">in</span> <span style=\"background-color:#f2f2ff\">a</span> <span style=\"background-color:#ff8a8a\">generic</span> <span style=\"background-color:#f0f0ff\">sense</span> <span style=\"background-color:#ffd6d6\">trying</span> <span style=\"background-color:#fefeff\">to</span> <span style=\"background-color:#f0f0ff\">avoid</span> <span style=\"background-color:#ff5959\">whenever</span> <span style=\"background-color:#f0f0ff\">possible</span> <span style=\"background-color:#f2f2ff\">a</span> <span style=\"background-color:#fffefe\">to</span> <span style=\"background-color:#e3e3ff\">specific</span> <span style=\"background-color:#ffecec\">or</span> <span style=\"background-color:#fcfcff\">the</span> <span style=\"background-color:#fafaff\">next</span> <span style=\"background-color:#fffefe\">section</span> <span style=\"background-color:#ff8a8a\">decomposition</span> <span style=\"background-color:#fffcfc\">as</span> <span style=\"background-color:#fcfcff\">a</span> <span style=\"background-color:#f8f8ff\">general</span> <span style=\"background-color:#fefeff\">concept</span> <span style=\"background-color:#fafaff\">will</span> <span style=\"background-color:#fffcfc\">explain</span> <span style=\"background-color:#fffefe\">the</span> <span style=\"background-color:#fff8f8\">basic</span> <span style=\"background-color:#f3f3ff\">approaches</span> <span style=\"background-color:#fffefe\">underlying</span> <span style=\"background-color:#fffefe\">the</span> <span style=\"background-color:#ffcccc\">decomposition</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fffefe\">in</span> <span style=\"background-color:#fffcfc\">section</span> <span style=\"background-color:#fffefe\">bag</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fefeff\">words</span> <span style=\"background-color:#f8f8ff\">models</span> <span style=\"background-color:#fffefe\">revisited</span> <span style=\"background-color:#fefeff\">we</span> <span style=\"background-color:#fcfcff\">will</span> <span style=\"background-color:#f0f0ff\">give</span> <span style=\"background-color:#fffefe\">a</span> <span style=\"background-color:#fff4f4\">short</span> <span style=\"background-color:#dcdcff\">recapitulation</span> <span style=\"background-color:#fffcfc\">about</span> <span style=\"background-color:#fffcfc\">bag</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fffefe\">words</span> <span style=\"background-color:#f6f6ff\">features</span> <span style=\"background-color:#fefeff\">and</span> <span style=\"background-color:#fffefe\">and</span> <span style=\"background-color:#fcfcff\">related</span> <span style=\"background-color:#fefeff\">work</span> <span style=\"background-color:#f0f0ff\">overview</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fffcfc\">the</span> <span style=\"background-color:#ffacac\">decomposition</span> <span style=\"background-color:#fffefe\">steps</span> <span style=\"background-color:#f8f8ff\">will</span> <span style=\"background-color:#e3e3ff\">discuss</span> <span style=\"background-color:#fffefe\">the</span> <span style=\"background-color:#ff0000\">decomposition</span> <span style=\"background-color:#fefeff\">of</span> <span style=\"background-color:#f3f3ff\">a</span> <span style=\"background-color:#fffafa\">into</span> <span style=\"background-color:#fff8f8\">sums</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#a6a6ff\">scores</span> <span style=\"background-color:#fff2f2\">over</span> <span style=\"background-color:#fefeff\">small</span> <span style=\"background-color:#fcfcff\">of</span> <span style=\"background-color:#fcfcff\">the</span> <span style=\"background-color:#e2e2ff\">image</span> <span style=\"background-color:#e0e0ff\">and</span> <span style=\"background-color:#fcfcff\">the</span> <span style=\"background-color:#ffdcdc\">projection</span> <span style=\"background-color:#ffcece\">down</span> <span style=\"background-color:#f2f2ff\">to</span> <span style=\"background-color:#c6c6ff\">single</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"\\nLRP heatmap:\")    \n",
    "display(HTML(html_heatmap(words, R_words)))                                    # display the heat map of relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LRP heatmap:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#f6f6ff\">we</span> <span style=\"background-color:#d0d0ff\">will</span> <span style=\"background-color:#ffecec\">consider</span> <span style=\"background-color:#dadaff\">both</span> <span style=\"background-color:#fefeff\">types</span> <span style=\"background-color:#f3f3ff\">of</span> <span style=\"background-color:#f6f6ff\">in</span> <span style=\"background-color:#f2f2ff\">a</span> <span style=\"background-color:#ff8a8a\">generic</span> <span style=\"background-color:#f0f0ff\">sense</span> <span style=\"background-color:#ffd6d6\">trying</span> <span style=\"background-color:#fefeff\">to</span> <span style=\"background-color:#f0f0ff\">avoid</span> <span style=\"background-color:#ff5959\">whenever</span> <span style=\"background-color:#f0f0ff\">possible</span> <span style=\"background-color:#f2f2ff\">a</span> <span style=\"background-color:#fffefe\">to</span> <span style=\"background-color:#e3e3ff\">specific</span> <span style=\"background-color:#ffecec\">or</span> <span style=\"background-color:#fcfcff\">the</span> <span style=\"background-color:#fafaff\">next</span> <span style=\"background-color:#fffefe\">section</span> <span style=\"background-color:#ff8a8a\">decomposition</span> <span style=\"background-color:#fffcfc\">as</span> <span style=\"background-color:#fcfcff\">a</span> <span style=\"background-color:#f8f8ff\">general</span> <span style=\"background-color:#fefeff\">concept</span> <span style=\"background-color:#fafaff\">will</span> <span style=\"background-color:#fffcfc\">explain</span> <span style=\"background-color:#fffefe\">the</span> <span style=\"background-color:#fff8f8\">basic</span> <span style=\"background-color:#f3f3ff\">approaches</span> <span style=\"background-color:#fffefe\">underlying</span> <span style=\"background-color:#fffefe\">the</span> <span style=\"background-color:#ffcccc\">decomposition</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fffefe\">in</span> <span style=\"background-color:#fffcfc\">section</span> <span style=\"background-color:#fffefe\">bag</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fefeff\">words</span> <span style=\"background-color:#f8f8ff\">models</span> <span style=\"background-color:#fffefe\">revisited</span> <span style=\"background-color:#fefeff\">we</span> <span style=\"background-color:#fcfcff\">will</span> <span style=\"background-color:#f0f0ff\">give</span> <span style=\"background-color:#fffefe\">a</span> <span style=\"background-color:#fff4f4\">short</span> <span style=\"background-color:#dcdcff\">recapitulation</span> <span style=\"background-color:#fffcfc\">about</span> <span style=\"background-color:#fffcfc\">bag</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fffefe\">words</span> <span style=\"background-color:#f6f6ff\">features</span> <span style=\"background-color:#fefeff\">and</span> <span style=\"background-color:#fffefe\">and</span> <span style=\"background-color:#fcfcff\">related</span> <span style=\"background-color:#fefeff\">work</span> <span style=\"background-color:#f0f0ff\">overview</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#fffcfc\">the</span> <span style=\"background-color:#ffacac\">decomposition</span> <span style=\"background-color:#fffefe\">steps</span> <span style=\"background-color:#f8f8ff\">will</span> <span style=\"background-color:#e3e3ff\">discuss</span> <span style=\"background-color:#fffefe\">the</span> <span style=\"background-color:#ff0000\">decomposition</span> <span style=\"background-color:#fefeff\">of</span> <span style=\"background-color:#f3f3ff\">a</span> <span style=\"background-color:#fffafa\">into</span> <span style=\"background-color:#fff8f8\">sums</span> <span style=\"background-color:#fffefe\">of</span> <span style=\"background-color:#a6a6ff\">scores</span> <span style=\"background-color:#fff2f2\">over</span> <span style=\"background-color:#fefeff\">small</span> <span style=\"background-color:#fcfcff\">of</span> <span style=\"background-color:#fcfcff\">the</span> <span style=\"background-color:#e2e2ff\">image</span> <span style=\"background-color:#e0e0ff\">and</span> <span style=\"background-color:#fcfcff\">the</span> <span style=\"background-color:#ffdcdc\">projection</span> <span style=\"background-color:#ffcece\">down</span> <span style=\"background-color:#f2f2ff\">to</span> <span style=\"background-color:#c6c6ff\">single</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_class = 1\n",
    "\n",
    "w_indices           = [net.voc.index(w) for w in words]                      # convert input sentence to word IDs\n",
    "Rx, Rx_rev, R_rest  = net.lrp(w_indices, target_class, eps, bias_factor)     # perform LRP\n",
    "R_words             = np.sum(Rx + Rx_rev, axis=1)                            # compute word-level LRP relevances\n",
    "\n",
    "scores              = net.s.copy()                                           # classification prediction scores\n",
    "print (\"\\nLRP heatmap:\")    \n",
    "display(HTML(html_heatmap(words, R_words)))                                  # display the heat map of relevances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** LRP can be used on more AI systems than sentiment analysis. Check out this demo, where you can use the same implementation of LRP on Handwriting Classification, Image Classification and Text Classification: https://lrpserver.hhi.fraunhofer.de/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Use the LRP to identify important words, and then remove them to see how the prediction changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Try other texts with different sentiment and contents. Try to fool the Neural Network with difficult texts, and then use the LRP to see what made it make a wrong prediction. "
   ]
  }
 ],
 "metadata": {
  "gist_id": "cd29264f4357c587ebca4d2b082df89f",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
